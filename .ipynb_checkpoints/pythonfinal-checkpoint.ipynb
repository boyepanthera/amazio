{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Hk9-B7oYNGp",
    "outputId": "6c3dfc2b-6419-49cc-8e86-f4dcc1acc90c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /opt/anaconda3/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from kagglehub) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->kagglehub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l1_qBJuacHX_",
    "outputId": "c7f2e87d-f968-401a-d9bc-8e3e4b74bd60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: kaggle\n",
      "unzip:  cannot find or open consumer-reviews-of-amazon-products.zip, consumer-reviews-of-amazon-products.zip.zip or consumer-reviews-of-amazon-products.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d datafiniti/consumer-reviews-of-amazon-products\n",
    "!unzip consumer-reviews-of-amazon-products.zip -d /dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5lIQ1edckUk",
    "outputId": "1941d6b6-f15b-4307-fd59-6566be1ebc70"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the main file (update if needed)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Inspect the dataset\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns in the dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the main file (update if needed)\n",
    "file_path = 'dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the dataset\n",
    "print(\"Columns in the dataset:\", df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--nB4-Gtd8oo",
    "outputId": "f38f1f1c-89b9-4e4f-9a41-6bff93745f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        reviews.text  \\\n",
      "0  I thought it would be as big as small paper bu...   \n",
      "1  This kindle is light and easy to use especiall...   \n",
      "2  Didnt know how much i'd use a kindle so went f...   \n",
      "3  I am 100 happy with my purchase. I caught it o...   \n",
      "4  Solid entry level Kindle. Great for kids. Gift...   \n",
      "\n",
      "                                    processed_review  \n",
      "0  thought would big small paper turn like palm t...  \n",
      "1             kindle light easy use especially beach  \n",
      "2  didnt know much use kindle went lower end im h...  \n",
      "3  happy purchase caught sale really good price n...  \n",
      "4  solid entry level kindle great kids gifted kid...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Define a tokenizer to match words (ignores punctuations)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Preprocessing function with RegexpTokenizer\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(str(text).lower())  # Tokenize and lowercase\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'reviews.text' column\n",
    "df['processed_review'] = df['reviews.text'].apply(preprocess_text)\n",
    "\n",
    "# Verify the new column\n",
    "print(df[['reviews.text', 'processed_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nLXPekC0guOO",
    "outputId": "252925a5-3ec2-4576-de75-08d5ada21bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    processed_review         label\n",
      "0  thought would big small paper turn like palm t...          True\n",
      "1             kindle light easy use especially beach  Unverifiable\n",
      "2  didnt know much use kindle went lower end im h...          True\n",
      "3  happy purchase caught sale really good price n...          True\n",
      "4  solid entry level kindle great kids gifted kid...          True\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Add synthetic labels (True, False, Unverifiable)\n",
    "labels = ['True', 'False', 'Unverifiable']\n",
    "df['label'] = [random.choice(labels) for _ in range(len(df))]\n",
    "\n",
    "# Verify the labeled dataset\n",
    "print(df[['processed_review', 'label']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQMtAVRWgzWD",
    "outputId": "b266a7df-0981-4f4f-ed39-747a55de1c62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and testing datasets created successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train, test = train_test_split(df[['processed_review', 'label']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the splits for future use\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n",
    "\n",
    "print(\"Training and testing datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M28Md7_ag3BL",
    "outputId": "85a77a8c-a410-4c4b-857a-fa91f03c2d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHzjTm27h4R7",
    "outputId": "ae38cc87-a611-4fc7-f078-36ab570abe11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: Index(['id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand',\n",
      "       'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer',\n",
      "       'manufacturerNumber', 'reviews.date', 'reviews.dateAdded',\n",
      "       'reviews.dateSeen', 'reviews.doRecommend', 'reviews.id',\n",
      "       'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs',\n",
      "       'reviews.text', 'reviews.title', 'reviews.username', 'sourceURLs'],\n",
      "      dtype='object')\n",
      "                     id             dateAdded           dateUpdated  \\\n",
      "0  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "1  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "2  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "3  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "4  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "\n",
      "                                                name       asins   brand  \\\n",
      "0  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "1  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "2  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "3  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "4  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "\n",
      "                                          categories primaryCategories  \\\n",
      "0  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "1  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "2  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "3  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "4  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "\n",
      "                                           imageURLs  \\\n",
      "0  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "1  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "2  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "3  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "4  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "\n",
      "                                                keys  ...  \\\n",
      "0  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "1  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "2  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "3  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "4  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "\n",
      "                                    reviews.dateSeen reviews.doRecommend  \\\n",
      "0  2018-05-27T00:00:00Z,2017-09-18T00:00:00Z,2017...               False   \n",
      "1  2018-05-27T00:00:00Z,2017-07-07T00:00:00Z,2017...                True   \n",
      "2                               2018-05-27T00:00:00Z                True   \n",
      "3                               2018-10-09T00:00:00Z                True   \n",
      "4                               2018-05-27T00:00:00Z                True   \n",
      "\n",
      "    reviews.id reviews.numHelpful reviews.rating  \\\n",
      "0          NaN                  0              3   \n",
      "1          NaN                  0              5   \n",
      "2          NaN                  0              4   \n",
      "3  177283626.0                  3              5   \n",
      "4          NaN                  0              5   \n",
      "\n",
      "                                  reviews.sourceURLs  \\\n",
      "0  http://reviews.bestbuy.com/3545/5442403/review...   \n",
      "1  http://reviews.bestbuy.com/3545/5442403/review...   \n",
      "2  https://reviews.bestbuy.com/3545/5442403/revie...   \n",
      "3  https://redsky.target.com/groot-domain-api/v1/...   \n",
      "4  https://reviews.bestbuy.com/3545/5442403/revie...   \n",
      "\n",
      "                                        reviews.text  \\\n",
      "0  I thought it would be as big as small paper bu...   \n",
      "1  This kindle is light and easy to use especiall...   \n",
      "2  Didnt know how much i'd use a kindle so went f...   \n",
      "3  I am 100 happy with my purchase. I caught it o...   \n",
      "4  Solid entry level Kindle. Great for kids. Gift...   \n",
      "\n",
      "                                  reviews.title  reviews.username  \\\n",
      "0                                     Too small            llyyue   \n",
      "1  Great light reader. Easy to use at the beach            Charmi   \n",
      "2                           Great for the price      johnnyjojojo   \n",
      "3                                   A Great Buy           Kdperry   \n",
      "4      Solid entry-level Kindle. Great for kids       Johnnyblack   \n",
      "\n",
      "                                          sourceURLs  \n",
      "0  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "1  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "2  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "3  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "4  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reload the dataset\n",
    "file_path = 'dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'  # Update the path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Verify the DataFrame\n",
    "print(\"Columns in the dataset:\", df.columns)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "id": "uwY3GtjHh71g",
    "outputId": "5c3c18f3-e590-4780-bcba-d23f0a5b85b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-97f74534b4e1>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Preprocess the 'reviews.text' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews.text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Verify the processed DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
      "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-97f74534b4e1>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Preprocessing function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Tokenize and lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Remove stopwords and non-alphabetic tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure necessary NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(str(text).lower())  # Tokenize and lowercase\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Remove stopwords and non-alphabetic tokens\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the 'reviews.text' column\n",
    "df['processed_review'] = df['reviews.text'].apply(preprocess_text)\n",
    "\n",
    "# Verify the processed DataFrame\n",
    "print(df[['reviews.text', 'processed_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "X6Jnqsz8iLrd",
    "outputId": "dcb87bf0-11a6-446a-e7ee-9e833c580d04"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "FileSystemPathPointer('/root/nltk_data/tokenizers/punkt')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Force re-download the punkt resource\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# Verify the punkt resource\n",
    "nltk.data.find('tokenizers/punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYRP_bcSiUbZ",
    "outputId": "c71da0b3-a617-4f19-e401-be12278c3644"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: nltk 3.9.1\n",
      "Uninstalling nltk-3.9.1:\n",
      "  Would remove:\n",
      "    /usr/local/bin/nltk\n",
      "    /usr/local/lib/python3.10/dist-packages/nltk-3.9.1.dist-info/*\n",
      "    /usr/local/lib/python3.10/dist-packages/nltk/*\n",
      "Proceed (Y/n)? Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/uninstall.py\", line 106, in run\n",
      "    uninstall_pathset = req.uninstall(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_install.py\", line 722, in uninstall\n",
      "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 364, in remove\n",
      "    if auto_confirm or self._allowed_to_proceed(verbose):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/req/req_uninstall.py\", line 404, in _allowed_to_proceed\n",
      "    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 235, in ask\n",
      "    response = input(message)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
      "    return run(options, args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
      "    logger.critical(\"Operation cancelled by user\")\n",
      "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1514, in critical\n",
      "    def critical(self, msg, *args, **kwargs):\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "pip uninstall nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "8zBkxI5MiX4M",
    "outputId": "d981f2f4-d27f-42e7-8e51-2820718e4349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.8.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "e792b31af6be4442b080ba9311f0edb2",
       "pip_warning": {
        "packages": [
         "nltk"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cu8rBSZYipPU",
    "outputId": "44b04a65-912d-488d-f49a-321122a5527d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OvIzh4bfM9f",
    "outputId": "6349524b-1ca0-4bf3-9798-556f5772e931"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Clear the existing NLTK data\n",
    "nltk.data.path = ['/root/nltk_data']\n",
    "nltk.download('punkt', force=True)  # Force a fresh download of punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZV_WKMNMeQht",
    "outputId": "650f2528-6c2e-4fc5-a872-64d830da8a7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the DataFrame: Index(['id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand',\n",
      "       'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer',\n",
      "       'manufacturerNumber', 'reviews.date', 'reviews.dateAdded',\n",
      "       'reviews.dateSeen', 'reviews.doRecommend', 'reviews.id',\n",
      "       'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs',\n",
      "       'reviews.text', 'reviews.title', 'reviews.username', 'sourceURLs',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in the DataFrame:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wlNRrdDVcm8y",
    "outputId": "fc7765dd-5235-4510-ecc0-a8bfeb087e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\n"
     ]
    }
   ],
   "source": [
    "ls dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rm4uIbCdkfpK",
    "outputId": "966b728b-dee3-459f-da1d-8d5e41ce1aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id             dateAdded           dateUpdated  \\\n",
      "0  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "1  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "2  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "3  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "4  AVqVGZNvQMlgsOJE6eUY  2017-03-03T16:56:05Z  2018-10-25T16:36:31Z   \n",
      "\n",
      "                                                name       asins   brand  \\\n",
      "0  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "1  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "2  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "3  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "4  Amazon Kindle E-Reader 6\" Wifi (8th Generation...  B00ZV9PXP2  Amazon   \n",
      "\n",
      "                                          categories primaryCategories  \\\n",
      "0  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "1  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "2  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "3  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "4  Computers,Electronics Features,Tablets,Electro...       Electronics   \n",
      "\n",
      "                                           imageURLs  \\\n",
      "0  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "1  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "2  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "3  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "4  https://pisces.bbystatic.com/image2/BestBuy_US...   \n",
      "\n",
      "                                                keys  ...  \\\n",
      "0  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "1  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "2  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "3  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "4  allnewkindleereaderblack6glarefreetouchscreend...  ...   \n",
      "\n",
      "                                    reviews.dateSeen reviews.doRecommend  \\\n",
      "0  2018-05-27T00:00:00Z,2017-09-18T00:00:00Z,2017...               False   \n",
      "1  2018-05-27T00:00:00Z,2017-07-07T00:00:00Z,2017...                True   \n",
      "2                               2018-05-27T00:00:00Z                True   \n",
      "3                               2018-10-09T00:00:00Z                True   \n",
      "4                               2018-05-27T00:00:00Z                True   \n",
      "\n",
      "    reviews.id reviews.numHelpful reviews.rating  \\\n",
      "0          NaN                  0              3   \n",
      "1          NaN                  0              5   \n",
      "2          NaN                  0              4   \n",
      "3  177283626.0                  3              5   \n",
      "4          NaN                  0              5   \n",
      "\n",
      "                                  reviews.sourceURLs  \\\n",
      "0  http://reviews.bestbuy.com/3545/5442403/review...   \n",
      "1  http://reviews.bestbuy.com/3545/5442403/review...   \n",
      "2  https://reviews.bestbuy.com/3545/5442403/revie...   \n",
      "3  https://redsky.target.com/groot-domain-api/v1/...   \n",
      "4  https://reviews.bestbuy.com/3545/5442403/revie...   \n",
      "\n",
      "                                        reviews.text  \\\n",
      "0  I thought it would be as big as small paper bu...   \n",
      "1  This kindle is light and easy to use especiall...   \n",
      "2  Didnt know how much i'd use a kindle so went f...   \n",
      "3  I am 100 happy with my purchase. I caught it o...   \n",
      "4  Solid entry level Kindle. Great for kids. Gift...   \n",
      "\n",
      "                                  reviews.title  reviews.username  \\\n",
      "0                                     Too small            llyyue   \n",
      "1  Great light reader. Easy to use at the beach            Charmi   \n",
      "2                           Great for the price      johnnyjojojo   \n",
      "3                                   A Great Buy           Kdperry   \n",
      "4      Solid entry-level Kindle. Great for kids       Johnnyblack   \n",
      "\n",
      "                                          sourceURLs  \n",
      "0  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "1  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "2  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "3  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "4  https://www.newegg.com/Product/Product.aspx%25...  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'  # Update this path as necessary\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to confirm successful loading\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJKa2Y__kyNe",
    "outputId": "ae905af8-f20d-419f-d22f-7d06f0a605f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        reviews.text  \\\n",
      "0  I thought it would be as big as small paper bu...   \n",
      "1  This kindle is light and easy to use especiall...   \n",
      "2  Didnt know how much i'd use a kindle so went f...   \n",
      "3  I am 100 happy with my purchase. I caught it o...   \n",
      "4  Solid entry level Kindle. Great for kids. Gift...   \n",
      "\n",
      "                                    processed_review  \n",
      "0  thought would big small paper turn like palm t...  \n",
      "1             kindle light easy use especially beach  \n",
      "2  didnt know much use kindle went lower end im h...  \n",
      "3  100 happy purchase caught sale really good pri...  \n",
      "4  solid entry level kindle great kids gifted kid...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Join tokens back into string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to 'reviews.text' column\n",
    "df['processed_review'] = df['reviews.text'].apply(preprocess_text)\n",
    "\n",
    "# Display the processed reviews\n",
    "print(df[['reviews.text', 'processed_review']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zC1qMfFnk28U",
    "outputId": "32a24564-86a5-46c7-e285-641d41a37b60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "positive    4686\n",
      "neutral      197\n",
      "negative     117\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a function to assign sentiment labels\n",
    "def assign_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 'positive'\n",
    "    elif rating == 3:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "\n",
    "# Apply the function to create a new 'sentiment' column\n",
    "df['sentiment'] = df['reviews.rating'].apply(assign_sentiment)\n",
    "\n",
    "# Display the sentiment distribution\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s36q7GGHk9Xy",
    "outputId": "80c8f684-d169-417d-87be-516d5e2d5651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 4000\n",
      "Testing set size: 1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "train, test = train_test_split(df[['processed_review', 'sentiment']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the size of each set\n",
    "print(f'Training set size: {len(train)}')\n",
    "print(f'Testing set size: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HlqXU5Fck_EH",
    "outputId": "54fd763e-ad52-4846-94d6-5352aea83470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4000, 4721)\n",
      "X_test shape: (1000, 4721)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = vectorizer.fit_transform(train['processed_review'])\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = vectorizer.transform(test['processed_review'])\n",
    "\n",
    "# Display the shape of the transformed data\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ATmfiG1lFlC",
    "outputId": "b597d440-f298-4332-df7c-15595fd18590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        24\n",
      "     neutral       1.00      0.03      0.05        39\n",
      "    positive       0.94      1.00      0.97       937\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.65      0.34      0.34      1000\n",
      "weighted avg       0.92      0.94      0.91      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, train['sentiment'])\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(test['sentiment'], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqi5kve3lSV9",
    "outputId": "3260b0f0-0ec8-4ed2-f58b-5cc37ede0542"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'dataset/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove punctuation and stopwords\n",
    "    words = [word for word in words if word.isalpha() and word not in stopwords.words('english')]\n",
    "    # Join words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['processed_review'] = df['reviews.text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OI36R26l6Cm",
    "outputId": "dad26fb2-61e9-4201-a893-139864b2a80f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    thought would big small paper turn like palm t...\n",
      "1               kindle light easy use especially beach\n",
      "2    didnt know much use kindle went lower end im h...\n",
      "3    happy purchase caught sale really good price n...\n",
      "4    solid entry level kindle great kids gifted kid...\n",
      "Name: processed_review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if 'processed_review' in df.columns:\n",
    "    print(df['processed_review'].head())\n",
    "else:\n",
    "    print(\"Column 'processed_review' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bL54Mot5mOk6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to map ratings to sentiments\n",
    "def map_rating_to_sentiment(rating):\n",
    "    if rating in [4, 5]:\n",
    "        return 'Positive'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    elif rating in [1, 2]:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Unknown'  # Handle unexpected ratings\n",
    "\n",
    "# Apply the function to create the 'label' column\n",
    "df['label'] = df['reviews.rating'].apply(map_rating_to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pgEdFSOuoGb9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'processed_review' contains the preprocessed text\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processed_review'], df['label'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uCrArmf_oJNu"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the training data; transform the test data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AsehBPPboKty",
    "outputId": "cb1ab160-a8ef-495d-ea76-242a22d0f5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Naive Bayes ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        24\n",
      "     Neutral       0.00      0.00      0.00        39\n",
      "    Positive       0.94      1.00      0.97       937\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.31      0.33      0.32      1000\n",
      "weighted avg       0.88      0.94      0.91      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        24\n",
      "     Neutral       1.00      0.03      0.05        39\n",
      "    Positive       0.94      1.00      0.97       937\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.65      0.34      0.34      1000\n",
      "weighted avg       0.92      0.94      0.91      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Support Vector Machine ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00        24\n",
      "     Neutral       1.00      0.05      0.10        39\n",
      "    Positive       0.94      1.00      0.97       937\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.65      0.35      0.36      1000\n",
      "weighted avg       0.92      0.94      0.91      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Random Forest ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.29      0.45        24\n",
      "     Neutral       0.91      0.26      0.40        39\n",
      "    Positive       0.95      1.00      0.98       937\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.95      0.52      0.61      1000\n",
      "weighted avg       0.95      0.95      0.94      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train_tfidf, y_train)\n",
    "    y_pred = clf.predict(X_test_tfidf)\n",
    "    print(f'--- {name} ---')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viDDZcW4lcbC",
    "outputId": "2d9c6146-4a74-43ce-b2b7-5ba70276c6e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'dateAdded', 'dateUpdated', 'name', 'asins', 'brand',\n",
      "       'categories', 'primaryCategories', 'imageURLs', 'keys', 'manufacturer',\n",
      "       'manufacturerNumber', 'reviews.date', 'reviews.dateAdded',\n",
      "       'reviews.dateSeen', 'reviews.doRecommend', 'reviews.id',\n",
      "       'reviews.numHelpful', 'reviews.rating', 'reviews.sourceURLs',\n",
      "       'reviews.text', 'reviews.title', 'reviews.username', 'sourceURLs'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
